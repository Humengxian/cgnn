{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Crystal Graph Neural Networks \u00b6 Crystal graph neural network (CGNN) architectures were developed for materials property predictions on the basis of a multi-graph representing each crystalline material in a materials database. A study on these architectures is presented in the paper \"Crystal Graph Neural Networks for Data Mining in Materials Science\" . This study used the PyTorch implementation available from the repository Tony-Y/cgnn . This site provides the user guide for the CGNN program. When you mention this work, please cite the CGNN paper: @techreport{yamamoto2019cgnn, Author = {Takenori Yamamoto}, Title = {Crystal Graph Neural Networks for Data Mining in Materials Science}, Address = {Yokohama, Japan}, Institution = {Research Institute for Mathematical and Computational Sciences, LLC}, Year = {2019}, Note = {https://github.com/Tony-Y/cgnn} }","title":"Home"},{"location":"#crystal-graph-neural-networks","text":"Crystal graph neural network (CGNN) architectures were developed for materials property predictions on the basis of a multi-graph representing each crystalline material in a materials database. A study on these architectures is presented in the paper \"Crystal Graph Neural Networks for Data Mining in Materials Science\" . This study used the PyTorch implementation available from the repository Tony-Y/cgnn . This site provides the user guide for the CGNN program. When you mention this work, please cite the CGNN paper: @techreport{yamamoto2019cgnn, Author = {Takenori Yamamoto}, Title = {Crystal Graph Neural Networks for Data Mining in Materials Science}, Address = {Yokohama, Japan}, Institution = {Research Institute for Mathematical and Computational Sciences, LLC}, Year = {2019}, Note = {https://github.com/Tony-Y/cgnn} }","title":"Crystal Graph Neural Networks"},{"location":"architectures/","text":"Architectures \u00b6 Embedding \u00b6 The i i -th initial hidden state h_{i}^{(0)} \\in \\mathbb{R}^{d_{h}} h_{i}^{(0)} \\in \\mathbb{R}^{d_{h}} is given by the embedding of the i i -th node state v_{i} v_{i} using the embedding matrix E \\in \\mathbb{R}^{d_{v} \\times d_{h}} E \\in \\mathbb{R}^{d_{v} \\times d_{h}} . The hidden states \\{ h_{i}^{(t)} \\}_{t=1}^{T} \\{ h_{i}^{(t)} \\}_{t=1}^{T} are sequentially produced by stacked convolution blocks (CB), as shown in the figure of a CGNN architecture below. Convolution Block \u00b6 The CB is composed of an edge neural network (EdgeNet), a gated convolution layer, and a multi-layer fully connected neural network (MFCNet), as shown below. The EdgeNet produces edge states e_{ij} \\in \\mathbb{R}^{d_{e}} e_{ij} \\in \\mathbb{R}^{d_{e}} . The CB output h_{i}^{\\rm out} h_{i}^{\\rm out} is the sum of the shortcut state h_{i}^{\\rm in} h_{i}^{\\rm in} and the MFCNet output. The EdgeNet and MFCNet are optional components. Multilayer Fully Connected Neural Networks \u00b6 The MFCNet is composed of L_{c} L_{c} layers, each of which is given by h^{\\rm out} = f(h^{\\rm in} W_{c}), h^{\\rm out} = f(h^{\\rm in} W_{c}), where W_{c} \\in \\mathbb{R}^{d_{h} \\times d_{h}} W_{c} \\in \\mathbb{R}^{d_{h} \\times d_{h}} denotes a weight matrix, and f(\\cdot) f(\\cdot) denotes an activate function. In neural network components presented below, f(\\cdot) f(\\cdot) appears repeatedly but is not needed to be the same activation function. Gated Convolution \u00b6 For i i -th hidden state, given a sequence of vectors \\{ h_{j}^{\\rm in} \\}_{j \\in \\mathcal{N}_{i}} \\{ h_{j}^{\\rm in} \\}_{j \\in \\mathcal{N}_{i}} , where h_{j}^{\\rm in} \\in \\mathbb{R}^{d_{c}} h_{j}^{\\rm in} \\in \\mathbb{R}^{d_{c}} is either a hidden state ( d_{c}=d_{h} d_{c}=d_{h} ) or an edge state ( d_{c}=d_{e} d_{c}=d_{e} ), the CB outputs h_{i}^{\\rm out} \\in \\mathbb{R}^{d_{h}} h_{i}^{\\rm out} \\in \\mathbb{R}^{d_{h}} , as shown below. The h_{i}^{\\rm out} h_{i}^{\\rm out} is given by h_{i}^{\\rm out} = \\sum_{j \\in {\\cal N}_{i}} \\sigma(h_{j}^{\\rm in} W_{cg}) \\odot f(h_{j}^{\\rm in} W_{ch}), h_{i}^{\\rm out} = \\sum_{j \\in {\\cal N}_{i}} \\sigma(h_{j}^{\\rm in} W_{cg}) \\odot f(h_{j}^{\\rm in} W_{ch}), where W_{cg} \\in \\mathbb{R}^{d_{c} \\times d_{h}} W_{cg} \\in \\mathbb{R}^{d_{c} \\times d_{h}} and W_{ch} \\in \\mathbb{R}^{d_{c} \\times d_{h}} W_{ch} \\in \\mathbb{R}^{d_{c} \\times d_{h}} denote weight matrices, \\sigma(\\cdot) \\sigma(\\cdot) denotes the sigmoid function, and \\odot \\odot element-wise multiplication. Edge Neural Networks \u00b6 The EdgeNet is a multi-layer neural network composed of L_{e} L_{e} layers, as shown below. Given i i -th hidden states h_{i} h_{i} and j j -th hidden state h_{j} h_{j} where j \\in \\mathcal{N}_{i} j \\in \\mathcal{N}_{i} , the EdgeNet outputs an edge state e_{ij} \\in \\mathbb{R}^{d_{e}} e_{ij} \\in \\mathbb{R}^{d_{e}} . Three variants of the EdgeNet layer are presented below. Original EdgeNet Layer \u00b6 The EdgeNet layer first developed, as shown below, is made of a bilinear transformation. It is expressed as $$ e_{ij}^{\\rm out} = f(\\mathcal{B}(h_{i}, e_{ij}^{\\rm in})), $$ and the bilinear transformation \\mathcal{B}(\\cdot,\\cdot) \\mathcal{B}(\\cdot,\\cdot) is defined by \\mathcal{B}(h, e) = h B e = \\left\\{ \\sum_{p, q} h(p)B(p,q,r)e(q) \\right\\}_{r=1}^{d_{e}}, \\mathcal{B}(h, e) = h B e = \\left\\{ \\sum_{p, q} h(p)B(p,q,r)e(q) \\right\\}_{r=1}^{d_{e}}, where B B is a weight tensor of order 3. Fast EdgeNet Layer \u00b6 The second EdgeNet layer is a fast version of \\mathcal{B}(\\cdot,\\cdot) \\mathcal{B}(\\cdot,\\cdot) , and is composed of two fully connected layers and the element-wise multiplication, as shown below. In the fast EdgeNet layer, the weight tensor is decomposed as B(p,q,r) = W_{he}(p,r) W_{ee}(q,r), B(p,q,r) = W_{he}(p,r) W_{ee}(q,r), where W_{he} W_{he} and W_{ee} W_{ee} denote weight matrices. Then, this layer is expressed as e_{ij}^{\\rm out} = f((h_{i}W_{he}) \\odot (e_{ij}^{\\rm in}W_{ee})). e_{ij}^{\\rm out} = f((h_{i}W_{he}) \\odot (e_{ij}^{\\rm in}W_{ee})). Moreover, the activation can be applied just after the two linear transformations, as expressed by e_{ij}^{\\rm out} = f(h_{i}W_{he}) \\odot f(e_{ij}^{\\rm in}W_{ee}). e_{ij}^{\\rm out} = f(h_{i}W_{he}) \\odot f(e_{ij}^{\\rm in}W_{ee}). Aggregate EdgeNet Layer \u00b6 The last EdgeNet layer is based on aggregated transformations \\sum_{l=1}^{C} \\mathcal{T}_{l}(h_{i}, e_{ij}^{\\rm in}) \\sum_{l=1}^{C} \\mathcal{T}_{l}(h_{i}, e_{ij}^{\\rm in}) , where C C is the cardinality, and \\mathcal{T}_{l} \\mathcal{T}_{l} is a bilinear transformation block (BTB), as shown below. As shown in the left panel of the figure above, \\mathcal{T}_{l} \\mathcal{T}_{l} is given by e_{ij}^{\\rm out} = \\mathcal{B}_{l}(f(h_{i} W_{hb}), f(e_{ij}^{\\rm in} W_{eb})) W_{be}, e_{ij}^{\\rm out} = \\mathcal{B}_{l}(f(h_{i} W_{hb}), f(e_{ij}^{\\rm in} W_{eb})) W_{be}, where \\mathcal{B}_{l} \\mathcal{B}_{l} denotes a bilinear transformation \\mathbb{R}^{d_{b}} \\times \\mathbb{R}^{d_{b}} \\to \\mathbb{R}^{d_{b}} \\mathbb{R}^{d_{b}} \\times \\mathbb{R}^{d_{b}} \\to \\mathbb{R}^{d_{b}} ( d_{b} \\cdot C \\approx d_{e} d_{b} \\cdot C \\approx d_{e} under normal use), and W_{hb} \\in \\mathbb{R}^{d_{h} \\times d_{b}} W_{hb} \\in \\mathbb{R}^{d_{h} \\times d_{b}} , W_{eb} \\in \\mathbb{R}^{d_{e} \\times d_{b}} W_{eb} \\in \\mathbb{R}^{d_{e} \\times d_{b}} , and W_{be} \\in \\mathbb{R}^{d_{b} \\times d_{e}} W_{be} \\in \\mathbb{R}^{d_{b} \\times d_{e}} denote weight matrices. As shown in the right panel, the aggregate EdgeNet layer outputs e_{ij}^{\\rm out} = f(\\sum_{l=1}^{C} \\mathcal{T}_{l}(h_{i}, e_{ij}^{\\rm in})). e_{ij}^{\\rm out} = f(\\sum_{l=1}^{C} \\mathcal{T}_{l}(h_{i}, e_{ij}^{\\rm in})). Edge Residual Neural Networks \u00b6 The EdgeNet becomes a residual neural network when every EdgeNet layer is wrapped by the EdgeResNet layer, as shown below. e_{ij}^{\\rm out} = e_{ij}^{\\rm in} W_{s} + \\mathfrak{E}(h_{i}, e_{ij}^{\\rm in}), e_{ij}^{\\rm out} = e_{ij}^{\\rm in} W_{s} + \\mathfrak{E}(h_{i}, e_{ij}^{\\rm in}), where W_{s} W_{s} denotes a weight matrix, and \\mathfrak{E}(\\cdot, \\cdot) \\mathfrak{E}(\\cdot, \\cdot) an EdgeNet layer. Pooling \u00b6 The graph-level representation \\Gamma^{(0)} \\in \\mathbb{R}^{d_{h}} \\Gamma^{(0)} \\in \\mathbb{R}^{d_{h}} is made from all the hidden states \\{ h_{i}^{(t)} \\}_{t=1,i=1}^{T,N} \\{ h_{i}^{(t)} \\}_{t=1,i=1}^{T,N} except for the initial ones. At each step t t , the hidden states \\{ h_{i}^{(t)} \\}_{i=1}^{N} \\{ h_{i}^{(t)} \\}_{i=1}^{N} are pooled with the gating mechanism as \\gamma_{t} = \\frac{1}{N} \\sum_{i=1}^{N} \\sigma(h_{i}^{(t)} W_{\\gamma}^{(t)} + b_{\\gamma}^{(t)}) \\odot h_{i}^{(t)}, \\gamma_{t} = \\frac{1}{N} \\sum_{i=1}^{N} \\sigma(h_{i}^{(t)} W_{\\gamma}^{(t)} + b_{\\gamma}^{(t)}) \\odot h_{i}^{(t)}, where W_{\\gamma}^{(t)} \\in \\mathbb{R}^{d_{h} \\times d_{h}} W_{\\gamma}^{(t)} \\in \\mathbb{R}^{d_{h} \\times d_{h}} denotes a weight matrix, and b_{\\gamma}^{(t)} \\in \\mathbb{R}^{d_{h}} b_{\\gamma}^{(t)} \\in \\mathbb{R}^{d_{h}} a bias vector. If the gating mechanism is not used, they are simply averaged as \\gamma_{t} = \\frac{1}{N} \\sum_{i=1}^{N} h_{i}^{(t)}. \\gamma_{t} = \\frac{1}{N} \\sum_{i=1}^{N} h_{i}^{(t)}. Then, the graph-level states \\gamma_{1},\\ldots,\\gamma_{T} \\gamma_{1},\\ldots,\\gamma_{T} are weightedly averaged as \\Gamma^{(0)} = f(\\sum_{t} \\gamma_{t}W_{\\Gamma}^{(t)}), \\Gamma^{(0)} = f(\\sum_{t} \\gamma_{t}W_{\\Gamma}^{(t)}), where W_{\\Gamma}^{(t)} \\in \\mathbb{R}^{d_{h} \\times d_{h}} W_{\\Gamma}^{(t)} \\in \\mathbb{R}^{d_{h} \\times d_{h}} denotes a weight matrix. If only the final graph-level state \\gamma_{T} \\gamma_{T} is used, it is simply activated as \\Gamma^{(0)} = f(\\gamma_{T}) \\Gamma^{(0)} = f(\\gamma_{T}) Graph-Level Neural Networks \u00b6 The graph-level MFCNet is composed of L_{g} L_{g} layers, each of which outputs \\Gamma^{\\rm out} \\in \\mathbb{R}^{d_{g}} \\Gamma^{\\rm out} \\in \\mathbb{R}^{d_{g}} given by \\Gamma^{\\rm out} = f(\\Gamma^{\\rm in} W_{g} + b_{g}), \\Gamma^{\\rm out} = f(\\Gamma^{\\rm in} W_{g} + b_{g}), where W_{g} W_{g} denotes a weight matrix, and b_{g} \\in \\mathbb{R}^{d_{g}} b_{g} \\in \\mathbb{R}^{d_{g}} a bias vector. For the first layer \\Gamma^{\\rm in} = \\Gamma^{(0)} \\Gamma^{\\rm in} = \\Gamma^{(0)} and W_{g} \\in \\mathbb{R}^{d_{h} \\times d_{g}} W_{g} \\in \\mathbb{R}^{d_{h} \\times d_{g}} , and otherwise \\Gamma^{\\rm in} \\in \\mathbb{R}^{d_{g}} \\Gamma^{\\rm in} \\in \\mathbb{R}^{d_{g}} and W_{g} \\in \\mathbb{R}^{d_{g} \\times d_{g}} W_{g} \\in \\mathbb{R}^{d_{g} \\times d_{g}} . The final layer's output \\Gamma^{(L_{g})} \\Gamma^{(L_{g})} is used as the input vector for the linear regression \\hat{y} = \\Gamma^{(L_{g})} \\cdot w_{r} + b_{r}, \\hat{y} = \\Gamma^{(L_{g})} \\cdot w_{r} + b_{r}, where w_{r} \\in \\mathbb{R}^{d_{g}} w_{r} \\in \\mathbb{R}^{d_{g}} denotes a weight vector, and b_{r} \\in \\mathbb{R} b_{r} \\in \\mathbb{R} a bias scalar. Given true values Y = \\{ y_{i} \\}_{i=1}^{N} Y = \\{ y_{i} \\}_{i=1}^{N} and predicted values \\hat{Y} = \\{ \\hat{y}_{i} \\}_{i=1}^{N} \\hat{Y} = \\{ \\hat{y}_{i} \\}_{i=1}^{N} , the mean squared error is calculated by L(Y, \\hat{Y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^{2}, L(Y, \\hat{Y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^{2}, which serves as the loss function for training a CGNN model. The mean absolute error (MAE) is also calculated by \\mathrm{MAE}(Y, \\hat{Y}) = \\frac{1}{N} \\sum_{i=1}^{N} | y_i - \\hat{y}_i |, \\mathrm{MAE}(Y, \\hat{Y}) = \\frac{1}{N} \\sum_{i=1}^{N} | y_i - \\hat{y}_i |, which is used as the validation metric to determine the best model in training. The root mean squared error \\sqrt{L(Y, \\hat{Y})} \\sqrt{L(Y, \\hat{Y})} is employed as an evaluation metric in testing as well as the MAE.","title":"Architectures"},{"location":"architectures/#architectures","text":"","title":"Architectures"},{"location":"architectures/#embedding","text":"The i i -th initial hidden state h_{i}^{(0)} \\in \\mathbb{R}^{d_{h}} h_{i}^{(0)} \\in \\mathbb{R}^{d_{h}} is given by the embedding of the i i -th node state v_{i} v_{i} using the embedding matrix E \\in \\mathbb{R}^{d_{v} \\times d_{h}} E \\in \\mathbb{R}^{d_{v} \\times d_{h}} . The hidden states \\{ h_{i}^{(t)} \\}_{t=1}^{T} \\{ h_{i}^{(t)} \\}_{t=1}^{T} are sequentially produced by stacked convolution blocks (CB), as shown in the figure of a CGNN architecture below.","title":"Embedding"},{"location":"architectures/#convolution-block","text":"The CB is composed of an edge neural network (EdgeNet), a gated convolution layer, and a multi-layer fully connected neural network (MFCNet), as shown below. The EdgeNet produces edge states e_{ij} \\in \\mathbb{R}^{d_{e}} e_{ij} \\in \\mathbb{R}^{d_{e}} . The CB output h_{i}^{\\rm out} h_{i}^{\\rm out} is the sum of the shortcut state h_{i}^{\\rm in} h_{i}^{\\rm in} and the MFCNet output. The EdgeNet and MFCNet are optional components.","title":"Convolution Block"},{"location":"architectures/#multilayer-fully-connected-neural-networks","text":"The MFCNet is composed of L_{c} L_{c} layers, each of which is given by h^{\\rm out} = f(h^{\\rm in} W_{c}), h^{\\rm out} = f(h^{\\rm in} W_{c}), where W_{c} \\in \\mathbb{R}^{d_{h} \\times d_{h}} W_{c} \\in \\mathbb{R}^{d_{h} \\times d_{h}} denotes a weight matrix, and f(\\cdot) f(\\cdot) denotes an activate function. In neural network components presented below, f(\\cdot) f(\\cdot) appears repeatedly but is not needed to be the same activation function.","title":"Multilayer Fully Connected Neural Networks"},{"location":"architectures/#gated-convolution","text":"For i i -th hidden state, given a sequence of vectors \\{ h_{j}^{\\rm in} \\}_{j \\in \\mathcal{N}_{i}} \\{ h_{j}^{\\rm in} \\}_{j \\in \\mathcal{N}_{i}} , where h_{j}^{\\rm in} \\in \\mathbb{R}^{d_{c}} h_{j}^{\\rm in} \\in \\mathbb{R}^{d_{c}} is either a hidden state ( d_{c}=d_{h} d_{c}=d_{h} ) or an edge state ( d_{c}=d_{e} d_{c}=d_{e} ), the CB outputs h_{i}^{\\rm out} \\in \\mathbb{R}^{d_{h}} h_{i}^{\\rm out} \\in \\mathbb{R}^{d_{h}} , as shown below. The h_{i}^{\\rm out} h_{i}^{\\rm out} is given by h_{i}^{\\rm out} = \\sum_{j \\in {\\cal N}_{i}} \\sigma(h_{j}^{\\rm in} W_{cg}) \\odot f(h_{j}^{\\rm in} W_{ch}), h_{i}^{\\rm out} = \\sum_{j \\in {\\cal N}_{i}} \\sigma(h_{j}^{\\rm in} W_{cg}) \\odot f(h_{j}^{\\rm in} W_{ch}), where W_{cg} \\in \\mathbb{R}^{d_{c} \\times d_{h}} W_{cg} \\in \\mathbb{R}^{d_{c} \\times d_{h}} and W_{ch} \\in \\mathbb{R}^{d_{c} \\times d_{h}} W_{ch} \\in \\mathbb{R}^{d_{c} \\times d_{h}} denote weight matrices, \\sigma(\\cdot) \\sigma(\\cdot) denotes the sigmoid function, and \\odot \\odot element-wise multiplication.","title":"Gated Convolution"},{"location":"architectures/#edge-neural-networks","text":"The EdgeNet is a multi-layer neural network composed of L_{e} L_{e} layers, as shown below. Given i i -th hidden states h_{i} h_{i} and j j -th hidden state h_{j} h_{j} where j \\in \\mathcal{N}_{i} j \\in \\mathcal{N}_{i} , the EdgeNet outputs an edge state e_{ij} \\in \\mathbb{R}^{d_{e}} e_{ij} \\in \\mathbb{R}^{d_{e}} . Three variants of the EdgeNet layer are presented below.","title":"Edge Neural Networks"},{"location":"architectures/#original-edgenet-layer","text":"The EdgeNet layer first developed, as shown below, is made of a bilinear transformation. It is expressed as $$ e_{ij}^{\\rm out} = f(\\mathcal{B}(h_{i}, e_{ij}^{\\rm in})), $$ and the bilinear transformation \\mathcal{B}(\\cdot,\\cdot) \\mathcal{B}(\\cdot,\\cdot) is defined by \\mathcal{B}(h, e) = h B e = \\left\\{ \\sum_{p, q} h(p)B(p,q,r)e(q) \\right\\}_{r=1}^{d_{e}}, \\mathcal{B}(h, e) = h B e = \\left\\{ \\sum_{p, q} h(p)B(p,q,r)e(q) \\right\\}_{r=1}^{d_{e}}, where B B is a weight tensor of order 3.","title":"Original EdgeNet Layer"},{"location":"architectures/#fast-edgenet-layer","text":"The second EdgeNet layer is a fast version of \\mathcal{B}(\\cdot,\\cdot) \\mathcal{B}(\\cdot,\\cdot) , and is composed of two fully connected layers and the element-wise multiplication, as shown below. In the fast EdgeNet layer, the weight tensor is decomposed as B(p,q,r) = W_{he}(p,r) W_{ee}(q,r), B(p,q,r) = W_{he}(p,r) W_{ee}(q,r), where W_{he} W_{he} and W_{ee} W_{ee} denote weight matrices. Then, this layer is expressed as e_{ij}^{\\rm out} = f((h_{i}W_{he}) \\odot (e_{ij}^{\\rm in}W_{ee})). e_{ij}^{\\rm out} = f((h_{i}W_{he}) \\odot (e_{ij}^{\\rm in}W_{ee})). Moreover, the activation can be applied just after the two linear transformations, as expressed by e_{ij}^{\\rm out} = f(h_{i}W_{he}) \\odot f(e_{ij}^{\\rm in}W_{ee}). e_{ij}^{\\rm out} = f(h_{i}W_{he}) \\odot f(e_{ij}^{\\rm in}W_{ee}).","title":"Fast EdgeNet Layer"},{"location":"architectures/#aggregate-edgenet-layer","text":"The last EdgeNet layer is based on aggregated transformations \\sum_{l=1}^{C} \\mathcal{T}_{l}(h_{i}, e_{ij}^{\\rm in}) \\sum_{l=1}^{C} \\mathcal{T}_{l}(h_{i}, e_{ij}^{\\rm in}) , where C C is the cardinality, and \\mathcal{T}_{l} \\mathcal{T}_{l} is a bilinear transformation block (BTB), as shown below. As shown in the left panel of the figure above, \\mathcal{T}_{l} \\mathcal{T}_{l} is given by e_{ij}^{\\rm out} = \\mathcal{B}_{l}(f(h_{i} W_{hb}), f(e_{ij}^{\\rm in} W_{eb})) W_{be}, e_{ij}^{\\rm out} = \\mathcal{B}_{l}(f(h_{i} W_{hb}), f(e_{ij}^{\\rm in} W_{eb})) W_{be}, where \\mathcal{B}_{l} \\mathcal{B}_{l} denotes a bilinear transformation \\mathbb{R}^{d_{b}} \\times \\mathbb{R}^{d_{b}} \\to \\mathbb{R}^{d_{b}} \\mathbb{R}^{d_{b}} \\times \\mathbb{R}^{d_{b}} \\to \\mathbb{R}^{d_{b}} ( d_{b} \\cdot C \\approx d_{e} d_{b} \\cdot C \\approx d_{e} under normal use), and W_{hb} \\in \\mathbb{R}^{d_{h} \\times d_{b}} W_{hb} \\in \\mathbb{R}^{d_{h} \\times d_{b}} , W_{eb} \\in \\mathbb{R}^{d_{e} \\times d_{b}} W_{eb} \\in \\mathbb{R}^{d_{e} \\times d_{b}} , and W_{be} \\in \\mathbb{R}^{d_{b} \\times d_{e}} W_{be} \\in \\mathbb{R}^{d_{b} \\times d_{e}} denote weight matrices. As shown in the right panel, the aggregate EdgeNet layer outputs e_{ij}^{\\rm out} = f(\\sum_{l=1}^{C} \\mathcal{T}_{l}(h_{i}, e_{ij}^{\\rm in})). e_{ij}^{\\rm out} = f(\\sum_{l=1}^{C} \\mathcal{T}_{l}(h_{i}, e_{ij}^{\\rm in})).","title":"Aggregate EdgeNet Layer"},{"location":"architectures/#edge-residual-neural-networks","text":"The EdgeNet becomes a residual neural network when every EdgeNet layer is wrapped by the EdgeResNet layer, as shown below. e_{ij}^{\\rm out} = e_{ij}^{\\rm in} W_{s} + \\mathfrak{E}(h_{i}, e_{ij}^{\\rm in}), e_{ij}^{\\rm out} = e_{ij}^{\\rm in} W_{s} + \\mathfrak{E}(h_{i}, e_{ij}^{\\rm in}), where W_{s} W_{s} denotes a weight matrix, and \\mathfrak{E}(\\cdot, \\cdot) \\mathfrak{E}(\\cdot, \\cdot) an EdgeNet layer.","title":"Edge Residual Neural Networks"},{"location":"architectures/#pooling","text":"The graph-level representation \\Gamma^{(0)} \\in \\mathbb{R}^{d_{h}} \\Gamma^{(0)} \\in \\mathbb{R}^{d_{h}} is made from all the hidden states \\{ h_{i}^{(t)} \\}_{t=1,i=1}^{T,N} \\{ h_{i}^{(t)} \\}_{t=1,i=1}^{T,N} except for the initial ones. At each step t t , the hidden states \\{ h_{i}^{(t)} \\}_{i=1}^{N} \\{ h_{i}^{(t)} \\}_{i=1}^{N} are pooled with the gating mechanism as \\gamma_{t} = \\frac{1}{N} \\sum_{i=1}^{N} \\sigma(h_{i}^{(t)} W_{\\gamma}^{(t)} + b_{\\gamma}^{(t)}) \\odot h_{i}^{(t)}, \\gamma_{t} = \\frac{1}{N} \\sum_{i=1}^{N} \\sigma(h_{i}^{(t)} W_{\\gamma}^{(t)} + b_{\\gamma}^{(t)}) \\odot h_{i}^{(t)}, where W_{\\gamma}^{(t)} \\in \\mathbb{R}^{d_{h} \\times d_{h}} W_{\\gamma}^{(t)} \\in \\mathbb{R}^{d_{h} \\times d_{h}} denotes a weight matrix, and b_{\\gamma}^{(t)} \\in \\mathbb{R}^{d_{h}} b_{\\gamma}^{(t)} \\in \\mathbb{R}^{d_{h}} a bias vector. If the gating mechanism is not used, they are simply averaged as \\gamma_{t} = \\frac{1}{N} \\sum_{i=1}^{N} h_{i}^{(t)}. \\gamma_{t} = \\frac{1}{N} \\sum_{i=1}^{N} h_{i}^{(t)}. Then, the graph-level states \\gamma_{1},\\ldots,\\gamma_{T} \\gamma_{1},\\ldots,\\gamma_{T} are weightedly averaged as \\Gamma^{(0)} = f(\\sum_{t} \\gamma_{t}W_{\\Gamma}^{(t)}), \\Gamma^{(0)} = f(\\sum_{t} \\gamma_{t}W_{\\Gamma}^{(t)}), where W_{\\Gamma}^{(t)} \\in \\mathbb{R}^{d_{h} \\times d_{h}} W_{\\Gamma}^{(t)} \\in \\mathbb{R}^{d_{h} \\times d_{h}} denotes a weight matrix. If only the final graph-level state \\gamma_{T} \\gamma_{T} is used, it is simply activated as \\Gamma^{(0)} = f(\\gamma_{T}) \\Gamma^{(0)} = f(\\gamma_{T})","title":"Pooling"},{"location":"architectures/#graph-level-neural-networks","text":"The graph-level MFCNet is composed of L_{g} L_{g} layers, each of which outputs \\Gamma^{\\rm out} \\in \\mathbb{R}^{d_{g}} \\Gamma^{\\rm out} \\in \\mathbb{R}^{d_{g}} given by \\Gamma^{\\rm out} = f(\\Gamma^{\\rm in} W_{g} + b_{g}), \\Gamma^{\\rm out} = f(\\Gamma^{\\rm in} W_{g} + b_{g}), where W_{g} W_{g} denotes a weight matrix, and b_{g} \\in \\mathbb{R}^{d_{g}} b_{g} \\in \\mathbb{R}^{d_{g}} a bias vector. For the first layer \\Gamma^{\\rm in} = \\Gamma^{(0)} \\Gamma^{\\rm in} = \\Gamma^{(0)} and W_{g} \\in \\mathbb{R}^{d_{h} \\times d_{g}} W_{g} \\in \\mathbb{R}^{d_{h} \\times d_{g}} , and otherwise \\Gamma^{\\rm in} \\in \\mathbb{R}^{d_{g}} \\Gamma^{\\rm in} \\in \\mathbb{R}^{d_{g}} and W_{g} \\in \\mathbb{R}^{d_{g} \\times d_{g}} W_{g} \\in \\mathbb{R}^{d_{g} \\times d_{g}} . The final layer's output \\Gamma^{(L_{g})} \\Gamma^{(L_{g})} is used as the input vector for the linear regression \\hat{y} = \\Gamma^{(L_{g})} \\cdot w_{r} + b_{r}, \\hat{y} = \\Gamma^{(L_{g})} \\cdot w_{r} + b_{r}, where w_{r} \\in \\mathbb{R}^{d_{g}} w_{r} \\in \\mathbb{R}^{d_{g}} denotes a weight vector, and b_{r} \\in \\mathbb{R} b_{r} \\in \\mathbb{R} a bias scalar. Given true values Y = \\{ y_{i} \\}_{i=1}^{N} Y = \\{ y_{i} \\}_{i=1}^{N} and predicted values \\hat{Y} = \\{ \\hat{y}_{i} \\}_{i=1}^{N} \\hat{Y} = \\{ \\hat{y}_{i} \\}_{i=1}^{N} , the mean squared error is calculated by L(Y, \\hat{Y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^{2}, L(Y, \\hat{Y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^{2}, which serves as the loss function for training a CGNN model. The mean absolute error (MAE) is also calculated by \\mathrm{MAE}(Y, \\hat{Y}) = \\frac{1}{N} \\sum_{i=1}^{N} | y_i - \\hat{y}_i |, \\mathrm{MAE}(Y, \\hat{Y}) = \\frac{1}{N} \\sum_{i=1}^{N} | y_i - \\hat{y}_i |, which is used as the validation metric to determine the best model in training. The root mean squared error \\sqrt{L(Y, \\hat{Y})} \\sqrt{L(Y, \\hat{Y})} is employed as an evaluation metric in testing as well as the MAE.","title":"Graph-Level Neural Networks"},{"location":"configuration/","text":"Configuration \u00b6 The CGNN program is a Python script, and you can run it with a basic configuration as follows: python ${ CGNN_HOME } /src/cgnn.py \\ --num_epochs 100 \\ --batch_size 512 \\ --lr 0 .001 \\ --n_node_feat ${ NodeFeatures } \\ --n_hidden_feat 64 \\ --n_graph_feat 128 \\ --n_conv 3 \\ --n_fc 2 \\ --dataset_path ${ DATASET } \\ --split_file ${ DATASET } /split.json \\ --target_name formation_energy_per_atom \\ --milestones 80 \\ --gamma 0 .1 \\ You can configure your CGNN model and training strategy using the following options: Device \u00b6 --device String (Default: cuda ) This string value must be cpu or cuda . If no CUDA device is available, the CPU device will be used. Features \u00b6 Node Features \u00b6 --n_node_feat Integer (Default: 4) This integer value is the number of node features, d_{v} d_{v} . If the one-hot encoding is used, it is the number of node species, K K . Attention The value must be equal to the size of the node vectors defined in the configuration file in the database directory ( config.json ). Hidden Features \u00b6 --n_hidden_feat Integer (Default: 16) This integer value is the number of features of the hidden states, d_{h} d_{h} . Graph Features \u00b6 --n_graph_feat Integer (Default: 32) This integer value is the number of features of the graph states, d_{g} d_{g} . EdgeNet Features \u00b6 --n_edge_net_feat Integer (Default: 16) This integer value is the number of features for the EdgeNet layers, d_{e} d_{e} . Convolution \u00b6 Convolution Blocks \u00b6 --n_conv Integer (Default: 3) This integer value is the number of convolution blocks, T T . Node-Level Activation \u00b6 --node_activation String (Default: none ) This string value is a name of the activation function lastly used in the convolution blocks. Node-Level Batch Normalization \u00b6 --use_node_batch_norm If this option is used, the batch normalization is applied before the node-level activation. Edge-Level Activation \u00b6 --edge_activation String (Default: none ) This string value is a name of the activation function used in the gated convolutions. Edge-Level Batch Normalization \u00b6 --use_edge_batch_norm If this option is used, the batch normalization is applied before the sigmoid and edge-level activation in the convolution. Convolution Type \u00b6 --conv_type Integer (Default: 0) If this value is greater than 0, the gated convolution is gvien by h_{i}^{\\rm out} = \\sum_{j \\in {\\cal N}_{i}} \\sigma(e_{ij}^{\\rm in} W_{cg}) \\odot f(h_{j}^{\\rm in} W_{ch}), h_{i}^{\\rm out} = \\sum_{j \\in {\\cal N}_{i}} \\sigma(e_{ij}^{\\rm in} W_{cg}) \\odot f(h_{j}^{\\rm in} W_{ch}), and otherwise the default h_{i}^{\\rm out} = \\sum_{j \\in {\\cal N}_{i}} \\sigma(e_{ij}^{\\rm in} W_{cg}) \\odot f(e_{ij}^{\\rm in} W_{ch}), h_{i}^{\\rm out} = \\sum_{j \\in {\\cal N}_{i}} \\sigma(e_{ij}^{\\rm in} W_{cg}) \\odot f(e_{ij}^{\\rm in} W_{ch}), where h_{j}^{\\rm in} h_{j}^{\\rm in} and e_{ij}^{\\rm in} e_{ij}^{\\rm in} are the input and output of the EdgeNet, respectively. Graph-Level MFCNet \u00b6 Graph-Level Layers \u00b6 --n_fc Integer (Default: 2) This integer value is the number of layers for the graph-level MFCNet, L_{g} L_{g} . Graph-Level Activation \u00b6 --activation String (Default: softplus ) This string value is a name of the activation function used in the graph-level fully connected layers and the pooling layer. Graph-Level Batch Normalization \u00b6 --use_batch_norm If this option is used, the batch normalization is applied before the graph-level activation, and the default bias terms are removed except for one in the linear regression. EdgeNet \u00b6 Warning When using an original or aggregate EdgeNet, the hidden and edge state sizes are practically limited to small numbers ( e.g. , d_{h}=16 d_{h}=16 and d_{e}=24 d_{e}=24 ) because the bilinear transformation used in the EdgeNet is an extremely time-consuming process. Example The original EdgeResNet with 2 layers is given by the following configuration: --n_edge_net_layers 2 \\ --use_edge_net_shortcut \\ EdgeNet Layers \u00b6 --n_edge_net_layers Integer (Default: 0) This integer value is the number of EdgeNet layers, L_{e} L_{e} . EdgeNet Activation \u00b6 --edge_net_activation String (Default: elu ) This string value is a name of the activation function used in the EdgeNet layers. EdgeNet Batch Normalization \u00b6 --use_edge_net_batch_norm If this option is used, the batch normalization is applied before the EdgeNet activation. Shortcut Option \u00b6 --use_edge_net_shortcut If this option is used, the EdgeResNet is employed. Fast EdgeNet \u00b6 Example The CGNN paper uses the fast EdgeResNet with only a single layer given by the following configuration: --n_edge_net_layers 1 \\ --use_fast_edge_network \\ --fast_edge_network_type 1 \\ --use_edge_net_shortcut \\ Fast EdgeNet Option \u00b6 --use_fast_edge_network If this option is used, one of the two fast EdgeNet is used in the convolution blocks. Fast EdgeNet Type \u00b6 --fast_edge_network_type Integer (Default: 0) If this value is 0, the original fast edge network is used, and otherwise the modified one. Aggregate EdgeNet \u00b6 Example The aggregate EdgeResNet with 2 layers for C=2 C=2 and d_{b}=12 d_{b}=12 is given by the following configuration: --n_edge_net_layers 2 \\ --use_aggregated_edge_network \\ --edge_net_cardinality 12 \\ --edge_net_width 2 \\ --use_edge_net_shortcut \\ Aggregate EdgeNet Option \u00b6 --use_aggregated_edge_network If this option is used, the aggregate EdgeNet is employed in the convolution blocks. EdgeNet Cardinality \u00b6 --edge_net_cardinality Integer (Default: 32) The integer value is the number of aggregated transformations (cardinality), C C . EdgeNet Width \u00b6 --edge_net_width Integer (Default: 4) The integer value is the feature size for all the bilinear transformations in the aggregate EdgeNet, d_{b} d_{b} . Convolution-Block MFCNet \u00b6 Example In the CGNN paper, the following configuration is used for the default convolution-block MFCNet. --n_postconv_net_layers 2 \\ --use_postconv_net_batch_norm \\ CB-MFCNet Layers \u00b6 --n_postconv_net_layers Integer (Default: 0) This integer value is the number of layers for the convolution-block MFCNet, L_{c} L_{c} . CB-MFCNet Activation \u00b6 --postconv_net_activation String (Default: elu ) The string value is a name of the activation function used in the convolution-block MFCNet layers. CB-MFCNet Batch Normalization \u00b6 --use_postconv_net_batch_norm If this option is used, the batch normalization is applied before the activation in every layer in the convolution-block MFCNet. Bias Terms \u00b6 Note The following bias terms are not included in the models used in the CGNN paper. Convolution Bias \u00b6 --conv_bias If this option is used, a bias term is added to every linear transformation in the gated convolution. EdgeNet Bias \u00b6 --edge_net_bias If this option is used, a bias term is added to every bilinear transformation in the original and aggregate EdgeNet, and to every linear transformation in the fast and aggregate EdgeNet. CB-MFCNet Bias \u00b6 --postconv_net_bias If this option is used, the bias term is added to the linear transformation in every CB-MFCNet layer. Pooling \u00b6 Example In the CGNN paper, the following configuration is used for the full and gated pooling. --full_pooling \\ --gated_pooling \\ Full Pooling \u00b6 --full_pooling If this option is used, the pooling layer uses all outputs of the convolution blocks to produce a graph-level state. Gated Pooling \u00b6 --gated_pooling If this option is used, the pooling layer employs the gating mechanism. Optimization \u00b6 Batch Size \u00b6 --batch_size Integer (Default: 8) The integer value is the mini-batch size for the stochastic optimization. Optimization Methods \u00b6 --optim String (Default: adam ) The string value is a name of the optimizer, which must be one of sgd , adam , and amsgrad . sgd is the stochastic gradient descent with the Nesterov momentum (the momentum factor = 0.9). adam and amsgrad use the standard parameters \\beta_{1}=0.9 \\beta_{1}=0.9 , \\beta_{2}=0.999 \\beta_{2}=0.999 , and \\epsilon=10^{-8} \\epsilon=10^{-8} . Learning Rate \u00b6 --lr Float (Default: 10^{-3} 10^{-3} ) This floating-point value is the learning rate for the stochastic optimization. Weight Decay \u00b6 --weight_decay Float (Default: 0) This floating-point value is the weight decay for the stochastic optimization ( i.e. , the L2 regularization). Gradient Clipping \u00b6 --clip_value Float (Default: 0) This floating-point value is used for the gradient clipping. Milestones \u00b6 --milestones Integer [Integer ...] (Default: 10) This integer sequence M_{1}, M_{2}, \\ldots, M_{n} M_{1}, M_{2}, \\ldots, M_{n} must satisfy the condition M_{i} < M_{i+1} M_{i} < M_{i+1} . At M_{i} M_{i} epochs, the learning rate is multiplied by \\gamma \\gamma . If the first value M_{1} M_{1} is negative, its absolute value will be used as the step size for the step LR scheduler. Learning Rate Decay \u00b6 --gamma Float (Default: 0.1) This floating-point value is used as the learning rate decay, which is the \\gamma \\gamma value of the LR scheduler. If you want to use the step LR scheduler, for example: --milestones -2 --gamma 0.98 which sets the step size to 2 epochs, and the \\gamma \\gamma value to 0.98. Cosine Annealing \u00b6 --cosine_annealing If this option is used, the cosine annealing scheduler is employed. \\eta_{t} = \\eta_{min} + \\frac{1}{2} (\\eta_{max} - \\eta_{min}) ( 1 + \\cos(\\frac{T_{cur}}{T_{max}}\\pi)) \\eta_{t} = \\eta_{min} + \\frac{1}{2} (\\eta_{max} - \\eta_{min}) ( 1 + \\cos(\\frac{T_{cur}}{T_{max}}\\pi)) \\eta_{min} \\eta_{min} and T_{max} T_{max} are set by the options --gamma and --milestones , respectively. \\eta_{max} \\eta_{max} is the learning rate set by the option --lr . Epochs \u00b6 --num_epochs Integer (Default: 5) This integer value is the total number of epochs for the stochastic optimization. Dataset \u00b6 Dataset Path \u00b6 --dataset_path String This string value must be a path to the directory containing dataset files, config.json , graph_data.npz , and targets.csv . Target \u00b6 --target_name String This string value must be one of the target names in the header of the target file targets.csv . Dataset Splitting \u00b6 --split_file String This string value must be a path to a split file split.json Workers \u00b6 --num_workers Integer (Default: 0) This value should be 0. Random Seed \u00b6 --seed Integer (Default: 12345) This value is the seed of the random number generator for Pytorch. Loading Model \u00b6 --load_model If this option is used, the initial model weights are loaded from a model file model.pth in the current directory. Extension \u00b6 --use_extension If this option is used, the extension layer multiplies the output of the regression layer by the number of nodes. This is usually used for extensive properties. Activation Functions \u00b6 The following keywords can be used as activation names. Softplus \u00b6 Keyword Softplus \\text{Softplus}(x) = \\log(1+\\exp(x)) \\text{Softplus}(x) = \\log(1+\\exp(x)) Shifted Softplus \u00b6 Keyword SSP \\text{SSP}(x) = \\text{Softplus}(x) \b- \\text{Softplus}(0) \\text{SSP}(x) = \\text{Softplus}(x) \b- \\text{Softplus}(0) Exponential Linear Units (ELU) \u00b6 Keyword ELU or 'ELU(alpha)' (default alpha = 1.0) \\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha (\\exp(x) \u2212 1)) \\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha (\\exp(x) \u2212 1)) Rectified Linear Units (ReLU) \u00b6 Keyword ReLU \\text{ReLU}(x) = \\max(0,x) \\text{ReLU}(x) = \\max(0,x) Scaled Exponential Linear Units (SELU) \u00b6 Keyword SELU \\text{SELU}(x) = \\lambda (\\max(0,x)+\\min(0,\\alpha (\\exp(x)\u22121))) \\text{SELU}(x) = \\lambda (\\max(0,x)+\\min(0,\\alpha (\\exp(x)\u22121))) Info Ref: \"Self-Normalizing Neural Networks\" arXiv Continuously Differentiable Exponential Linear Units (CELU) \u00b6 Keyword CELU or 'CELU(alpha)' (default alpha = 1.0) \\text{CELU}(x)=\\max(0,x)+\\min(0,\\alpha (\\exp(x/\\alpha)\u22121)) \\text{CELU}(x)=\\max(0,x)+\\min(0,\\alpha (\\exp(x/\\alpha)\u22121)) Info Ref: \"Continuously Differentiable Exponential Linear Units\" arXiv The Identity Activation \u00b6 Keyword None \\text{Identity}(x) = x \\text{Identity}(x) = x This is unavailable for --activation and --postconv_net_activation .","title":"Configuration"},{"location":"configuration/#configuration","text":"The CGNN program is a Python script, and you can run it with a basic configuration as follows: python ${ CGNN_HOME } /src/cgnn.py \\ --num_epochs 100 \\ --batch_size 512 \\ --lr 0 .001 \\ --n_node_feat ${ NodeFeatures } \\ --n_hidden_feat 64 \\ --n_graph_feat 128 \\ --n_conv 3 \\ --n_fc 2 \\ --dataset_path ${ DATASET } \\ --split_file ${ DATASET } /split.json \\ --target_name formation_energy_per_atom \\ --milestones 80 \\ --gamma 0 .1 \\ You can configure your CGNN model and training strategy using the following options:","title":"Configuration"},{"location":"configuration/#device","text":"--device String (Default: cuda ) This string value must be cpu or cuda . If no CUDA device is available, the CPU device will be used.","title":"Device"},{"location":"configuration/#features","text":"","title":"Features"},{"location":"configuration/#node-features","text":"--n_node_feat Integer (Default: 4) This integer value is the number of node features, d_{v} d_{v} . If the one-hot encoding is used, it is the number of node species, K K . Attention The value must be equal to the size of the node vectors defined in the configuration file in the database directory ( config.json ).","title":"Node Features"},{"location":"configuration/#hidden-features","text":"--n_hidden_feat Integer (Default: 16) This integer value is the number of features of the hidden states, d_{h} d_{h} .","title":"Hidden Features"},{"location":"configuration/#graph-features","text":"--n_graph_feat Integer (Default: 32) This integer value is the number of features of the graph states, d_{g} d_{g} .","title":"Graph Features"},{"location":"configuration/#edgenet-features","text":"--n_edge_net_feat Integer (Default: 16) This integer value is the number of features for the EdgeNet layers, d_{e} d_{e} .","title":"EdgeNet Features"},{"location":"configuration/#convolution","text":"","title":"Convolution"},{"location":"configuration/#convolution-blocks","text":"--n_conv Integer (Default: 3) This integer value is the number of convolution blocks, T T .","title":"Convolution Blocks"},{"location":"configuration/#node-level-activation","text":"--node_activation String (Default: none ) This string value is a name of the activation function lastly used in the convolution blocks.","title":"Node-Level Activation"},{"location":"configuration/#node-level-batch-normalization","text":"--use_node_batch_norm If this option is used, the batch normalization is applied before the node-level activation.","title":"Node-Level Batch Normalization"},{"location":"configuration/#edge-level-activation","text":"--edge_activation String (Default: none ) This string value is a name of the activation function used in the gated convolutions.","title":"Edge-Level Activation"},{"location":"configuration/#edge-level-batch-normalization","text":"--use_edge_batch_norm If this option is used, the batch normalization is applied before the sigmoid and edge-level activation in the convolution.","title":"Edge-Level Batch Normalization"},{"location":"configuration/#convolution-type","text":"--conv_type Integer (Default: 0) If this value is greater than 0, the gated convolution is gvien by h_{i}^{\\rm out} = \\sum_{j \\in {\\cal N}_{i}} \\sigma(e_{ij}^{\\rm in} W_{cg}) \\odot f(h_{j}^{\\rm in} W_{ch}), h_{i}^{\\rm out} = \\sum_{j \\in {\\cal N}_{i}} \\sigma(e_{ij}^{\\rm in} W_{cg}) \\odot f(h_{j}^{\\rm in} W_{ch}), and otherwise the default h_{i}^{\\rm out} = \\sum_{j \\in {\\cal N}_{i}} \\sigma(e_{ij}^{\\rm in} W_{cg}) \\odot f(e_{ij}^{\\rm in} W_{ch}), h_{i}^{\\rm out} = \\sum_{j \\in {\\cal N}_{i}} \\sigma(e_{ij}^{\\rm in} W_{cg}) \\odot f(e_{ij}^{\\rm in} W_{ch}), where h_{j}^{\\rm in} h_{j}^{\\rm in} and e_{ij}^{\\rm in} e_{ij}^{\\rm in} are the input and output of the EdgeNet, respectively.","title":"Convolution Type"},{"location":"configuration/#graph-level-mfcnet","text":"","title":"Graph-Level MFCNet"},{"location":"configuration/#graph-level-layers","text":"--n_fc Integer (Default: 2) This integer value is the number of layers for the graph-level MFCNet, L_{g} L_{g} .","title":"Graph-Level Layers"},{"location":"configuration/#graph-level-activation","text":"--activation String (Default: softplus ) This string value is a name of the activation function used in the graph-level fully connected layers and the pooling layer.","title":"Graph-Level Activation"},{"location":"configuration/#graph-level-batch-normalization","text":"--use_batch_norm If this option is used, the batch normalization is applied before the graph-level activation, and the default bias terms are removed except for one in the linear regression.","title":"Graph-Level Batch Normalization"},{"location":"configuration/#edgenet","text":"Warning When using an original or aggregate EdgeNet, the hidden and edge state sizes are practically limited to small numbers ( e.g. , d_{h}=16 d_{h}=16 and d_{e}=24 d_{e}=24 ) because the bilinear transformation used in the EdgeNet is an extremely time-consuming process. Example The original EdgeResNet with 2 layers is given by the following configuration: --n_edge_net_layers 2 \\ --use_edge_net_shortcut \\","title":"EdgeNet"},{"location":"configuration/#edgenet-layers","text":"--n_edge_net_layers Integer (Default: 0) This integer value is the number of EdgeNet layers, L_{e} L_{e} .","title":"EdgeNet Layers"},{"location":"configuration/#edgenet-activation","text":"--edge_net_activation String (Default: elu ) This string value is a name of the activation function used in the EdgeNet layers.","title":"EdgeNet Activation"},{"location":"configuration/#edgenet-batch-normalization","text":"--use_edge_net_batch_norm If this option is used, the batch normalization is applied before the EdgeNet activation.","title":"EdgeNet Batch Normalization"},{"location":"configuration/#shortcut-option","text":"--use_edge_net_shortcut If this option is used, the EdgeResNet is employed.","title":"Shortcut Option"},{"location":"configuration/#fast-edgenet","text":"Example The CGNN paper uses the fast EdgeResNet with only a single layer given by the following configuration: --n_edge_net_layers 1 \\ --use_fast_edge_network \\ --fast_edge_network_type 1 \\ --use_edge_net_shortcut \\","title":"Fast EdgeNet"},{"location":"configuration/#fast-edgenet-option","text":"--use_fast_edge_network If this option is used, one of the two fast EdgeNet is used in the convolution blocks.","title":"Fast EdgeNet Option"},{"location":"configuration/#fast-edgenet-type","text":"--fast_edge_network_type Integer (Default: 0) If this value is 0, the original fast edge network is used, and otherwise the modified one.","title":"Fast EdgeNet Type"},{"location":"configuration/#aggregate-edgenet","text":"Example The aggregate EdgeResNet with 2 layers for C=2 C=2 and d_{b}=12 d_{b}=12 is given by the following configuration: --n_edge_net_layers 2 \\ --use_aggregated_edge_network \\ --edge_net_cardinality 12 \\ --edge_net_width 2 \\ --use_edge_net_shortcut \\","title":"Aggregate EdgeNet"},{"location":"configuration/#aggregate-edgenet-option","text":"--use_aggregated_edge_network If this option is used, the aggregate EdgeNet is employed in the convolution blocks.","title":"Aggregate EdgeNet Option"},{"location":"configuration/#edgenet-cardinality","text":"--edge_net_cardinality Integer (Default: 32) The integer value is the number of aggregated transformations (cardinality), C C .","title":"EdgeNet Cardinality"},{"location":"configuration/#edgenet-width","text":"--edge_net_width Integer (Default: 4) The integer value is the feature size for all the bilinear transformations in the aggregate EdgeNet, d_{b} d_{b} .","title":"EdgeNet Width"},{"location":"configuration/#convolution-block-mfcnet","text":"Example In the CGNN paper, the following configuration is used for the default convolution-block MFCNet. --n_postconv_net_layers 2 \\ --use_postconv_net_batch_norm \\","title":"Convolution-Block MFCNet"},{"location":"configuration/#cb-mfcnet-layers","text":"--n_postconv_net_layers Integer (Default: 0) This integer value is the number of layers for the convolution-block MFCNet, L_{c} L_{c} .","title":"CB-MFCNet Layers"},{"location":"configuration/#cb-mfcnet-activation","text":"--postconv_net_activation String (Default: elu ) The string value is a name of the activation function used in the convolution-block MFCNet layers.","title":"CB-MFCNet Activation"},{"location":"configuration/#cb-mfcnet-batch-normalization","text":"--use_postconv_net_batch_norm If this option is used, the batch normalization is applied before the activation in every layer in the convolution-block MFCNet.","title":"CB-MFCNet Batch Normalization"},{"location":"configuration/#bias-terms","text":"Note The following bias terms are not included in the models used in the CGNN paper.","title":"Bias Terms"},{"location":"configuration/#convolution-bias","text":"--conv_bias If this option is used, a bias term is added to every linear transformation in the gated convolution.","title":"Convolution Bias"},{"location":"configuration/#edgenet-bias","text":"--edge_net_bias If this option is used, a bias term is added to every bilinear transformation in the original and aggregate EdgeNet, and to every linear transformation in the fast and aggregate EdgeNet.","title":"EdgeNet Bias"},{"location":"configuration/#cb-mfcnet-bias","text":"--postconv_net_bias If this option is used, the bias term is added to the linear transformation in every CB-MFCNet layer.","title":"CB-MFCNet Bias"},{"location":"configuration/#pooling","text":"Example In the CGNN paper, the following configuration is used for the full and gated pooling. --full_pooling \\ --gated_pooling \\","title":"Pooling"},{"location":"configuration/#full-pooling","text":"--full_pooling If this option is used, the pooling layer uses all outputs of the convolution blocks to produce a graph-level state.","title":"Full Pooling"},{"location":"configuration/#gated-pooling","text":"--gated_pooling If this option is used, the pooling layer employs the gating mechanism.","title":"Gated Pooling"},{"location":"configuration/#optimization","text":"","title":"Optimization"},{"location":"configuration/#batch-size","text":"--batch_size Integer (Default: 8) The integer value is the mini-batch size for the stochastic optimization.","title":"Batch Size"},{"location":"configuration/#optimization-methods","text":"--optim String (Default: adam ) The string value is a name of the optimizer, which must be one of sgd , adam , and amsgrad . sgd is the stochastic gradient descent with the Nesterov momentum (the momentum factor = 0.9). adam and amsgrad use the standard parameters \\beta_{1}=0.9 \\beta_{1}=0.9 , \\beta_{2}=0.999 \\beta_{2}=0.999 , and \\epsilon=10^{-8} \\epsilon=10^{-8} .","title":"Optimization Methods"},{"location":"configuration/#learning-rate","text":"--lr Float (Default: 10^{-3} 10^{-3} ) This floating-point value is the learning rate for the stochastic optimization.","title":"Learning Rate"},{"location":"configuration/#weight-decay","text":"--weight_decay Float (Default: 0) This floating-point value is the weight decay for the stochastic optimization ( i.e. , the L2 regularization).","title":"Weight Decay"},{"location":"configuration/#gradient-clipping","text":"--clip_value Float (Default: 0) This floating-point value is used for the gradient clipping.","title":"Gradient Clipping"},{"location":"configuration/#milestones","text":"--milestones Integer [Integer ...] (Default: 10) This integer sequence M_{1}, M_{2}, \\ldots, M_{n} M_{1}, M_{2}, \\ldots, M_{n} must satisfy the condition M_{i} < M_{i+1} M_{i} < M_{i+1} . At M_{i} M_{i} epochs, the learning rate is multiplied by \\gamma \\gamma . If the first value M_{1} M_{1} is negative, its absolute value will be used as the step size for the step LR scheduler.","title":"Milestones"},{"location":"configuration/#learning-rate-decay","text":"--gamma Float (Default: 0.1) This floating-point value is used as the learning rate decay, which is the \\gamma \\gamma value of the LR scheduler. If you want to use the step LR scheduler, for example: --milestones -2 --gamma 0.98 which sets the step size to 2 epochs, and the \\gamma \\gamma value to 0.98.","title":"Learning Rate Decay"},{"location":"configuration/#cosine-annealing","text":"--cosine_annealing If this option is used, the cosine annealing scheduler is employed. \\eta_{t} = \\eta_{min} + \\frac{1}{2} (\\eta_{max} - \\eta_{min}) ( 1 + \\cos(\\frac{T_{cur}}{T_{max}}\\pi)) \\eta_{t} = \\eta_{min} + \\frac{1}{2} (\\eta_{max} - \\eta_{min}) ( 1 + \\cos(\\frac{T_{cur}}{T_{max}}\\pi)) \\eta_{min} \\eta_{min} and T_{max} T_{max} are set by the options --gamma and --milestones , respectively. \\eta_{max} \\eta_{max} is the learning rate set by the option --lr .","title":"Cosine Annealing"},{"location":"configuration/#epochs","text":"--num_epochs Integer (Default: 5) This integer value is the total number of epochs for the stochastic optimization.","title":"Epochs"},{"location":"configuration/#dataset","text":"","title":"Dataset"},{"location":"configuration/#dataset-path","text":"--dataset_path String This string value must be a path to the directory containing dataset files, config.json , graph_data.npz , and targets.csv .","title":"Dataset Path"},{"location":"configuration/#target","text":"--target_name String This string value must be one of the target names in the header of the target file targets.csv .","title":"Target"},{"location":"configuration/#dataset-splitting","text":"--split_file String This string value must be a path to a split file split.json","title":"Dataset Splitting"},{"location":"configuration/#workers","text":"--num_workers Integer (Default: 0) This value should be 0.","title":"Workers"},{"location":"configuration/#random-seed","text":"--seed Integer (Default: 12345) This value is the seed of the random number generator for Pytorch.","title":"Random Seed"},{"location":"configuration/#loading-model","text":"--load_model If this option is used, the initial model weights are loaded from a model file model.pth in the current directory.","title":"Loading Model"},{"location":"configuration/#extension","text":"--use_extension If this option is used, the extension layer multiplies the output of the regression layer by the number of nodes. This is usually used for extensive properties.","title":"Extension"},{"location":"configuration/#activation-functions","text":"The following keywords can be used as activation names.","title":"Activation Functions"},{"location":"configuration/#softplus","text":"Keyword Softplus \\text{Softplus}(x) = \\log(1+\\exp(x)) \\text{Softplus}(x) = \\log(1+\\exp(x))","title":"Softplus"},{"location":"configuration/#shifted-softplus","text":"Keyword SSP \\text{SSP}(x) = \\text{Softplus}(x) \b- \\text{Softplus}(0) \\text{SSP}(x) = \\text{Softplus}(x) \b- \\text{Softplus}(0)","title":"Shifted Softplus"},{"location":"configuration/#exponential-linear-units-elu","text":"Keyword ELU or 'ELU(alpha)' (default alpha = 1.0) \\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha (\\exp(x) \u2212 1)) \\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha (\\exp(x) \u2212 1))","title":"Exponential Linear Units (ELU)"},{"location":"configuration/#rectified-linear-units-relu","text":"Keyword ReLU \\text{ReLU}(x) = \\max(0,x) \\text{ReLU}(x) = \\max(0,x)","title":"Rectified Linear Units (ReLU)"},{"location":"configuration/#scaled-exponential-linear-units-selu","text":"Keyword SELU \\text{SELU}(x) = \\lambda (\\max(0,x)+\\min(0,\\alpha (\\exp(x)\u22121))) \\text{SELU}(x) = \\lambda (\\max(0,x)+\\min(0,\\alpha (\\exp(x)\u22121))) Info Ref: \"Self-Normalizing Neural Networks\" arXiv","title":"Scaled Exponential Linear Units (SELU)"},{"location":"configuration/#continuously-differentiable-exponential-linear-units-celu","text":"Keyword CELU or 'CELU(alpha)' (default alpha = 1.0) \\text{CELU}(x)=\\max(0,x)+\\min(0,\\alpha (\\exp(x/\\alpha)\u22121)) \\text{CELU}(x)=\\max(0,x)+\\min(0,\\alpha (\\exp(x/\\alpha)\u22121)) Info Ref: \"Continuously Differentiable Exponential Linear Units\" arXiv","title":"Continuously Differentiable Exponential Linear Units (CELU)"},{"location":"configuration/#the-identity-activation","text":"Keyword None \\text{Identity}(x) = x \\text{Identity}(x) = x This is unavailable for --activation and --postconv_net_activation .","title":"The Identity Activation"},{"location":"introduction/","text":"Introduction \u00b6 Interacting systems, molecules, and biopolymers can be represented topologically as graphs. A crystalline material may be represented topologically as a multi-graph, which is called a crystal graph. A method to create crystal graphs is proposed in the CGNN paper, and its Python implementation ( tools/mp_graph.py ) is available from the repository Tony-Y/cgnn . Example The structure (left) and crystal graph (right) for a form of \\mathrm{SiO}_{2} \\mathrm{SiO}_{2} : Crystal Graphs \u00b6 A crystal graph is composed of a set of nodes and a multi-set of directed edges. Given a sequence of nodes with a label, the i i -th node's label can be mapped to an integer k_{i} \\in [0\\mathrel{{.}\\,{.}}K-1] k_{i} \\in [0\\mathrel{{.}\\,{.}}K-1] , where K K denotes the number of node species, and then the node sequence can be encoded into an integer sequence \\{ k_{i} \\}_{i=1}^{N} \\{ k_{i} \\}_{i=1}^{N} , where N N denotes the number of nodes. Since a directed edge is defined by a source and target node, the multi-set of nodes neighboring the i i -th node is defined by the complete multi-set of target nodes for the i i -th node serving as a source node, and can be encoded into the index list of the target nodes \\mathcal{N}_{i} \\mathcal{N}_{i} . Thus, the crystal graph may be represented by the pair of the integer sequence and the sequence of the neighbor lists (\\{ k_{i} \\}_{i=1}^{N}, \\{ \\mathcal{N}_{i} \\}_{i=1}^{N}) (\\{ k_{i} \\}_{i=1}^{N}, \\{ \\mathcal{N}_{i} \\}_{i=1}^{N}) . Example The OQMD contains 89 chemical elements (1 to 83, and 89 to 94 in atomic number), which can be labeled with integer numbers k \\in [0\\mathrel{{.}\\,{.}}88] k \\in [0\\mathrel{{.}\\,{.}}88] in the ascending order of atomic number. Since the atomic numbers of Si and O are 14 and 8, respectively, the crystal graph of \\mathrm{SiO}_{2} \\mathrm{SiO}_{2} in the previous example is represented by the pair of the integer sequence [13, 13, 7, 7, 7, 7] and the list of neighbor lists [ [2, 3, 4, 5], [2, 3, 4, 5], [0, 1], [0, 1], [0, 1], [0, 1], ] Node species may have some features including its integer label. The node vector can be made from those features, and thus we have the set of K K node vectors. The integer sequence \\{ k_{i} \\}_{i=1}^{N} \\{ k_{i} \\}_{i=1}^{N} is mapped to a sequence of vectors \\{ v_{i} \\}_{i=1}^{N} \\{ v_{i} \\}_{i=1}^{N} , where v_{i} \\in \\mathbb{R}^{d_{v}} v_{i} \\in \\mathbb{R}^{d_{v}} is the k_{i} k_{i} -th node vector. The sequence of node vectors is used as an input for a CGNN model. Example If the materials dataset used has only 3 chemical elements, Ti (titanium), O (oxygen), and S (sulfur), their node vectors can be represented using the one-hot encoding as follows: Ti : [1, 0, 0] O : [0, 1, 0] S : [0, 0, 1] Then, using the indexing $$ \\mathrm{Ti} \\to 0; \\mathrm{O} \\to 1; \\mathrm{S} \\to 2 $$ the list of node vectors is given by the following: [ [1, 0, 0], [0, 1, 0], [0, 0, 1], ] Elemental properties, which are available from a chemical elements database ( e.g. , WebElements ), can be added to the node vectors as numerical features. For instance, the Pauling electronegativity for each element, obtained from WebElements, is shown below: Element Electronegativity Ti 1.54 O 3.44 S 2.58 When the electronegativity is added to the node vector, the list of node vectors is given by the following: [ [1, 0, 0, 1.54], [0, 1, 0, 3.44], [0, 0, 1, 2.58], ] Dataset \u00b6 The CGNN program needs the following files: targets.csv consists of all target values. graph_data.npz composed of all node and neighbor lists of graphs. config.json defines node vectors. split.json defines data splitting (train/val/test). Target Values \u00b6 targets.csv must have a header row consisting name and target names. The name column must store identifiers like an ID number or string that is unique to each example in the dataset. The target columns must store numerical values excluding NaN and None . Graph Data \u00b6 You can create a graph data file ( graph_data.npz ) as follows: graphs = dict () for name , structure in dataset : nodes = ... # A species-index list neighbors = ... # A list of neighbor lists graphs [ name ] = ( nodes , neighbors ) np . savez_compressed ( 'graph_data.npz' , graph_dict = graphs ) where the object structure stores structural information used to create a graph with nodes \\{ k_{i} \\}_{i=1}^{N} \\{ k_{i} \\}_{i=1}^{N} and neighbors \\{ \\mathcal{N}_{i} \\}_{i=1}^{N} \\{ \\mathcal{N}_{i} \\}_{i=1}^{N} , and name is the same identifier as in targets.csv for each example. Node Vectors \u00b6 You can create a configuration file ( config.json ) as follows: config = dict () config [ \"node_vectors\" ] = ... # A list of node feature lists with open ( \"config.json\" , 'w' ) as f : json . dump ( config , f ) When using the one-hot encoding, the node vectors are given by config [ \"node_vectors\" ] = np . eye ( n_species , n_species ) . tolist () where n_species denotes the number of node species, K K . Data Splitting \u00b6 You can create a data-splitting file ( split.json ) as follows: split = dict () split [ \"train\" ] = ... # The index list for the training set split [ \"val\" ] = ... # The index list for the validation set split [ \"test\" ] = ... # The index list for the testing set with open ( \"split.json\" , 'w' ) as f : json . dump ( split , f ) where the index, which must be a non-negative integer, is a row label of the data frame that the CSV file targets.csv is read into.","title":"Introduction"},{"location":"introduction/#introduction","text":"Interacting systems, molecules, and biopolymers can be represented topologically as graphs. A crystalline material may be represented topologically as a multi-graph, which is called a crystal graph. A method to create crystal graphs is proposed in the CGNN paper, and its Python implementation ( tools/mp_graph.py ) is available from the repository Tony-Y/cgnn . Example The structure (left) and crystal graph (right) for a form of \\mathrm{SiO}_{2} \\mathrm{SiO}_{2} :","title":"Introduction"},{"location":"introduction/#crystal-graphs","text":"A crystal graph is composed of a set of nodes and a multi-set of directed edges. Given a sequence of nodes with a label, the i i -th node's label can be mapped to an integer k_{i} \\in [0\\mathrel{{.}\\,{.}}K-1] k_{i} \\in [0\\mathrel{{.}\\,{.}}K-1] , where K K denotes the number of node species, and then the node sequence can be encoded into an integer sequence \\{ k_{i} \\}_{i=1}^{N} \\{ k_{i} \\}_{i=1}^{N} , where N N denotes the number of nodes. Since a directed edge is defined by a source and target node, the multi-set of nodes neighboring the i i -th node is defined by the complete multi-set of target nodes for the i i -th node serving as a source node, and can be encoded into the index list of the target nodes \\mathcal{N}_{i} \\mathcal{N}_{i} . Thus, the crystal graph may be represented by the pair of the integer sequence and the sequence of the neighbor lists (\\{ k_{i} \\}_{i=1}^{N}, \\{ \\mathcal{N}_{i} \\}_{i=1}^{N}) (\\{ k_{i} \\}_{i=1}^{N}, \\{ \\mathcal{N}_{i} \\}_{i=1}^{N}) . Example The OQMD contains 89 chemical elements (1 to 83, and 89 to 94 in atomic number), which can be labeled with integer numbers k \\in [0\\mathrel{{.}\\,{.}}88] k \\in [0\\mathrel{{.}\\,{.}}88] in the ascending order of atomic number. Since the atomic numbers of Si and O are 14 and 8, respectively, the crystal graph of \\mathrm{SiO}_{2} \\mathrm{SiO}_{2} in the previous example is represented by the pair of the integer sequence [13, 13, 7, 7, 7, 7] and the list of neighbor lists [ [2, 3, 4, 5], [2, 3, 4, 5], [0, 1], [0, 1], [0, 1], [0, 1], ] Node species may have some features including its integer label. The node vector can be made from those features, and thus we have the set of K K node vectors. The integer sequence \\{ k_{i} \\}_{i=1}^{N} \\{ k_{i} \\}_{i=1}^{N} is mapped to a sequence of vectors \\{ v_{i} \\}_{i=1}^{N} \\{ v_{i} \\}_{i=1}^{N} , where v_{i} \\in \\mathbb{R}^{d_{v}} v_{i} \\in \\mathbb{R}^{d_{v}} is the k_{i} k_{i} -th node vector. The sequence of node vectors is used as an input for a CGNN model. Example If the materials dataset used has only 3 chemical elements, Ti (titanium), O (oxygen), and S (sulfur), their node vectors can be represented using the one-hot encoding as follows: Ti : [1, 0, 0] O : [0, 1, 0] S : [0, 0, 1] Then, using the indexing $$ \\mathrm{Ti} \\to 0; \\mathrm{O} \\to 1; \\mathrm{S} \\to 2 $$ the list of node vectors is given by the following: [ [1, 0, 0], [0, 1, 0], [0, 0, 1], ] Elemental properties, which are available from a chemical elements database ( e.g. , WebElements ), can be added to the node vectors as numerical features. For instance, the Pauling electronegativity for each element, obtained from WebElements, is shown below: Element Electronegativity Ti 1.54 O 3.44 S 2.58 When the electronegativity is added to the node vector, the list of node vectors is given by the following: [ [1, 0, 0, 1.54], [0, 1, 0, 3.44], [0, 0, 1, 2.58], ]","title":"Crystal Graphs"},{"location":"introduction/#dataset","text":"The CGNN program needs the following files: targets.csv consists of all target values. graph_data.npz composed of all node and neighbor lists of graphs. config.json defines node vectors. split.json defines data splitting (train/val/test).","title":"Dataset"},{"location":"introduction/#target-values","text":"targets.csv must have a header row consisting name and target names. The name column must store identifiers like an ID number or string that is unique to each example in the dataset. The target columns must store numerical values excluding NaN and None .","title":"Target Values"},{"location":"introduction/#graph-data","text":"You can create a graph data file ( graph_data.npz ) as follows: graphs = dict () for name , structure in dataset : nodes = ... # A species-index list neighbors = ... # A list of neighbor lists graphs [ name ] = ( nodes , neighbors ) np . savez_compressed ( 'graph_data.npz' , graph_dict = graphs ) where the object structure stores structural information used to create a graph with nodes \\{ k_{i} \\}_{i=1}^{N} \\{ k_{i} \\}_{i=1}^{N} and neighbors \\{ \\mathcal{N}_{i} \\}_{i=1}^{N} \\{ \\mathcal{N}_{i} \\}_{i=1}^{N} , and name is the same identifier as in targets.csv for each example.","title":"Graph Data"},{"location":"introduction/#node-vectors","text":"You can create a configuration file ( config.json ) as follows: config = dict () config [ \"node_vectors\" ] = ... # A list of node feature lists with open ( \"config.json\" , 'w' ) as f : json . dump ( config , f ) When using the one-hot encoding, the node vectors are given by config [ \"node_vectors\" ] = np . eye ( n_species , n_species ) . tolist () where n_species denotes the number of node species, K K .","title":"Node Vectors"},{"location":"introduction/#data-splitting","text":"You can create a data-splitting file ( split.json ) as follows: split = dict () split [ \"train\" ] = ... # The index list for the training set split [ \"val\" ] = ... # The index list for the validation set split [ \"test\" ] = ... # The index list for the testing set with open ( \"split.json\" , 'w' ) as f : json . dump ( split , f ) where the index, which must be a non-negative integer, is a row label of the data frame that the CSV file targets.csv is read into.","title":"Data Splitting"},{"location":"license/","text":"The CGNN source code is released under the Apache License 2.0: Copyright 2019 Takenori Yamamoto All rights reserved. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright 2019 Takenori Yamamoto Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"}]}